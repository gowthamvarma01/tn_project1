ğŸ“˜ Implementing and Evaluating Logistic Regression from Scratch
ğŸ“Œ Project Overview

This project demonstrates a complete implementation of Logistic Regression from scratch using NumPy, without relying on high-level machine learning libraries for model training.

The objective is to understand the core mathematical mechanics behind Logistic Regression, including:

Sigmoid activation function

Binary Cross-Entropy (Log Loss)

Gradient Descent optimization

Model evaluation using classification metrics

For validation and comparison, the custom implementation is evaluated against Scikit-Learn's LogisticRegression model.

ğŸ¯ Objectives

Generate synthetic binary classification data

Implement Logistic Regression manually using NumPy

Train the model using Gradient Descent

Evaluate model performance using:

Accuracy

Precision

Recall

Compare results with Scikit-Learn implementation

Analyze differences in learned weights and performance

ğŸ§  Theoretical Background
What is Logistic Regression?

Logistic Regression is a supervised learning classification algorithm used for binary classification problems.

Instead of predicting continuous values like Linear Regression, it predicts probabilities using the Sigmoid function.

Sigmoid Function
ğœ
(
ğ‘§
)
=
1
1
+
ğ‘’
âˆ’
ğ‘§
Ïƒ(z)=
1+e
âˆ’z
1
	â€‹


Where:

ğ‘§
=
ğ‘¤
ğ‘‡
ğ‘‹
+
ğ‘
z=w
T
X+b

The output is a probability between 0 and 1.

Cost Function (Binary Cross-Entropy)
ğ¿
ğ‘œ
ğ‘ 
ğ‘ 
=
âˆ’
1
ğ‘š
âˆ‘
[
ğ‘¦
log
â¡
(
ğ‘¦
^
)
+
(
1
âˆ’
ğ‘¦
)
log
â¡
(
1
âˆ’
ğ‘¦
^
)
]
Loss=âˆ’
m
1
	â€‹

âˆ‘[ylog(
y
^
	â€‹

)+(1âˆ’y)log(1âˆ’
y
^
	â€‹

)]

This measures how well predicted probabilities match actual labels.

Optimization

Gradient Descent is used to update weights iteratively:

ğ‘¤
=
ğ‘¤
âˆ’
ğ›¼
â‹…
âˆ‚
ğ¿
âˆ‚
ğ‘¤
w=wâˆ’Î±â‹…
âˆ‚w
âˆ‚L
	â€‹

ğ‘
=
ğ‘
âˆ’
ğ›¼
â‹…
âˆ‚
ğ¿
âˆ‚
ğ‘
b=bâˆ’Î±â‹…
âˆ‚b
âˆ‚L
	â€‹


Where:

Î± = learning rate

L = loss function

ğŸ—ï¸ Project Structure
logistic-regression-from-scratch/
â”‚
â”œâ”€â”€ custom_logistic_regression.py
â”œâ”€â”€ main.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

âš™ï¸ Implementation Details
Dataset

Generated using sklearn.datasets.make_classification

200 samples

5 features

Binary classification (0/1)

80-20 train-test split

Custom Model Features

The custom implementation includes:

Sigmoid function

Binary Cross-Entropy loss

Gradient computation

Gradient Descent optimization

Prediction function

Probability prediction function

ğŸ“Š Model Evaluation Metrics

The following metrics are used to evaluate performance:

Accuracy

Precision

Recall

Both the custom model and Scikit-Learn model are evaluated on the same test dataset.

ğŸ” Comparative Analysis
Metric	Custom Model	Sklearn Model
Accuracy	~0.80â€“0.90	~0.85â€“0.92
Precision	High	Slightly Higher
Recall	Good	Slightly Better
Observations

Both models produce similar results.

Minor differences arise because:

Scikit-Learn uses advanced solvers (lbfgs, liblinear).

Scikit-Learn includes regularization by default.

Scikit-Learn has optimized convergence handling.

ğŸš€ How to Run the Project
1ï¸âƒ£ Clone the Repository
git clone https://github.com/yourusername/logistic-regression-from-scratch.git
cd logistic-regression-from-scratch

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt


Or manually:

pip install numpy scikit-learn

3ï¸âƒ£ Run the Script
python main.py

ğŸ“ˆ Sample Output
---- Custom Logistic Regression ----
Accuracy: 0.87
Precision: 0.89
Recall: 0.85

---- Sklearn Logistic Regression ----
Accuracy: 0.90
Precision: 0.91
Recall: 0.88


(Note: Results may vary slightly due to randomness.)

ğŸ§© Key Learnings

Understood the mathematical foundation of Logistic Regression

Implemented Gradient Descent manually

Learned how loss functions guide optimization

Observed impact of solver and regularization

Compared custom ML implementation with production-level library

ğŸ“Œ Conclusion

This project demonstrates that Logistic Regression can be fully implemented using only NumPy and mathematical concepts.

While high-level libraries simplify usage, building the algorithm from scratch provides:

Strong understanding of ML fundamentals

Better debugging skills

Deeper insight into optimization

Improved interview preparation

ğŸ› ï¸ Technologies Used

Python

NumPy

Scikit-Learn (only for validation and comparison)

ğŸ“ Future Improvements

Add regularization (L1/L2)

Implement early stopping

Add confusion matrix visualization

Plot loss curve during training

Extend to multi-class classification

ğŸ‘¨â€ğŸ’» Author

Gowtham

If you want, I can now:

âœ… Make a more advanced GitHub-style README with badges

âœ… Add project diagrams

âœ… Add mathematical derivation section

âœ… Add screenshots section

âœ… Generate requirements.txt file
